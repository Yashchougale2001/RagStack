RAG_Stack: CPU-Friendly Retrieval-Augmented Generation System

A lightweight, modular Retrieval-Augmented Generation (RAG) system designed specifically for CPU-only environments and low-resource machines.

This project demonstrates how to build a complete, local RAG pipeline using:

ChromaDB for persistent vector storage

MiniLM embeddings for semantic retrieval

FLAN-T5 (small) for grounded answer generation

The system answers questions strictly from a local knowledge base, avoiding hallucinations.

ğŸ¯ Why This Project Exists

Most RAG tutorials assume:

Paid APIs

High-end GPUs

Cloud infrastructure

This project proves that:

You can build a real RAG system locally, for free, on a CPU with limited RAM, while still following clean architecture and production-style design.

ğŸš€ Features

Semantic retrieval using all-MiniLM-L6-v2

Persistent ChromaDB vector store (no re-embedding on restart)

CPU-friendly text generation using google/flan-t5-small

Overlapping text chunking for better retrieval quality

Strict context-grounded answering (no hallucinations)

Clean, modular, interview-ready codebase

Interactive CLI-based Q&A loop

User Query â†’ Retriever â†’ Generator â†’ Privacy Agent â†’ Final Answer + Privacy Report


ğŸ—‚ Project Structure
mini_rag/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ knowledge.txt              # Your raw text knowledge base
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader/
â”‚   â”‚   â””â”€â”€ loader.py              # load_text() & chunk_text()
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ vector_store.py        # create_vector_store(), add_chunks_to_db(), query_chunks()
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py           # retrieve_chunks() with min_docs & optional similarity filtering
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ generator.py           # generate_answer(), trim_context(), model init
â”‚   â”‚   â””â”€â”€ prompt_templates.py    # optional, store complex prompts separately
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ evaluator.py           # evaluate_answer() with grounding check
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest.py                  # Script to load text, chunk, and add to vector store
â”‚   â””â”€â”€ main.py                    # CLI entry point for QA
â”‚
â”œâ”€â”€ chroma_db/                     # Persistent vector DB (gitignored)
â”‚
â”œâ”€â”€ config.yaml                     # All configurable params (chunk_size, models, etc.)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”„ RAG Workflow (High-Level)

Load text from knowledge.txt

Split text into overlapping chunks

Generate embeddings using MiniLM

Store embeddings in ChromaDB

Accept user query

Embed query

Retrieve top-K relevant chunks

Generate answer using retrieved context only

ğŸ§  Architecture Overview
knowledge.txt
      â†“
Text Loader
      â†“
Chunking (with overlap)
      â†“
Embedding Model (MiniLM)
      â†“
Vector Store (ChromaDB â€“ persistent)
      â†“
User Query
      â†“
Query Embedding
      â†“
Similarity Search
      â†“
Top-K Relevant Chunks
      â†“
LLM (FLAN-T5)
      â†“
Final Answer

ğŸ§© Modular Design (File-Level Responsibility)
Module	Responsibility
loader	Load raw text and split into chunks
vector_store	Manage vector DB creation, insertion, retrieval
retriever	Query orchestration & retrieval logic
generator	Context-grounded answer generation
main	Application entry point & CLI loop

Each component has a single responsibility, making the system easy to extend or replace.

âš™ï¸ Tech Stack
Component	Technology
Embeddings	SentenceTransformers (MiniLM)
Vector DB	ChromaDB
LLM	google/flan-t5-small
Language	Python
Hardware	CPU-only
ğŸ§ª Limitations

Designed for small to medium text corpora

FLAN-T5-small has limited reasoning depth

No conversation memory (single-turn Q&A)

No re-ranking or hybrid search (yet)

ğŸ”® Future Improvements

Conversation memory

Similarity thresholding

Re-ranking (cross-encoder)

Hybrid retrieval (BM25 + embeddings)

Evaluation metrics for retrieval quality

Web UI / API layer

ğŸ“Œ Key Takeaway

This project demonstrates that RAG is an architecture, not a paid API feature.
It focuses on clarity, correctness, and constraints, making it ideal for learning, interviews, and small-scale applications.
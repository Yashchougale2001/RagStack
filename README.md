# Mini-RAG: CPU-Friendly Retrieval-Augmented Generation System

A lightweight, modular **Retrieval-Augmented Generation (RAG)** system built for **CPU-only environments**.  
It uses **ChromaDB** for vector storage, **MiniLM embeddings** for semantic search, and a **local FLAN-T5 model** for answer generation.  

This project demonstrates a **full RAG pipeline** with **modular, maintainable code**, suitable for small to medium text knowledge bases.

---

## ðŸš€ Features

- **Semantic retrieval** using `all-MiniLM-L6-v2` embeddings
- **ChromaDB vector database** for scalable, persistent storage
- **CPU-friendly LLM generation** with `google/flan-t5-small`
- **Proper chunking with overlap** for improved retrieval quality
- Modular structure: easy to maintain, extend, or swap components
- CLI-based interactive Q&A loop

---

## ðŸ—‚ Project Structure

mini_rag/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ knowledge.txt # Knowledge base (text)
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ loader.py # Loading + chunking text
â”‚ â”œâ”€â”€ vector_store.py # ChromaDB init, add, query
â”‚ â”œâ”€â”€ retrieval.py # Retrieval logic (optional thresholds)
â”‚ â”œâ”€â”€ generator.py # LLM prompt + answer generation
â”‚ â””â”€â”€ main.py # Main interactive CLI
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

Answers questions strictly from a local text file.

1. Load text
2. Split into chunks
3. Embed chunks
4. Store embeddings
5. Accept user query
6. Embed query
7. Retrieve relevant chunks
8. Ask LLM using retrieved text

Workflow

knowledge.txt
      â†“
Text Loader
      â†“
Text Chunking
      â†“
Embedding Model (MiniLM)
      â†“
Vector Store (in-memory)
      â†“
User Query
      â†“
Query Embedding
      â†“
Similarity Search (cosine)
      â†“
Top-K Relevant Chunks
      â†“
Displayed Answer (retrieved text)


## Architecture Overview

### File-Level Workflow
This project follows a modular RAG architecture where each component has a single responsibility.

![RAG File Workflow](docs/rag_file_level_workflow.png)

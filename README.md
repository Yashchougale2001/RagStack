RAG_Stack: CPU-Friendly Local Retrieval-Augmented Generation System

A lightweight, modular Retrieval-Augmented Generation (RAG) system designed for CPU-only, low-resource machines, using fully local models with zero paid APIs.

This project demonstrates how to build a production-style RAG pipeline that runs entirely on your machine using:

ChromaDB for persistent vector storage

BGE Small embeddings for accurate semantic retrieval

TinyLlama via Ollama for grounded answer generation

The system answers questions strictly from a local knowledge base, with explicit safeguards against hallucination.

ğŸ¯ Why This Project Exists

This project proves that:

RAG is an architecture, not a paid service

You can run a complete RAG pipeline locally, on CPU

Clean design and grounding rules matter more than model size

This is built for learning, interviews, and real-world constraints, not demo fluff.

ğŸš€ Features

Semantic retrieval using BAAI/bge-small-en-v1.5

Persistent ChromaDB (no re-embedding on restart)

Local LLM inference using TinyLlama via Ollama

Overlapping text chunking for better recall

Strict context-grounded answering

Explicit â€œI donâ€™t knowâ€ handling (no guessing)

Answer faithfulness evaluation

Clean, modular, interview-ready architecture

Interactive CLI-based Q&A

ğŸ—‚ Project Structure
mini_rag/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ knowledge.txt              # Raw knowledge base
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader/
â”‚   â”‚   â””â”€â”€ loader.py              # Text loading & chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ vector_store.py        # ChromaDB creation & queries
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py           # Top-K retrieval logic
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ generator.py           # Ollama + TinyLlama inference
â”‚   â”‚   â””â”€â”€ prompt_templates.py    # Strict grounding prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ evaluator.py           # Faithfulness checks
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest.py                  # One-time ingestion script
â”‚   â””â”€â”€ main.py                    # CLI entry point
â”‚
â”œâ”€â”€ chroma_db/                     # Persistent vector store
â”œâ”€â”€ config.yaml                    # Models, chunking, retrieval params
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”„ RAG Workflow (High-Level)

Load text from knowledge.txt

Split text into overlapping chunks

Generate embeddings using BGE Small

Store vectors in ChromaDB

Accept user query

Embed query

Retrieve top-K relevant chunks

Generate answer using TinyLlama

Validate answer grounding

ğŸ§  Architecture Overview
knowledge.txt
      â†“
Text Loader
      â†“
Chunking (overlap)
      â†“
Embedding Model (BGE Small)
      â†“
ChromaDB (Persistent)
      â†“
User Query
      â†“
Query Embedding
      â†“
Similarity Search
      â†“
Top-K Context Chunks
      â†“
TinyLlama (Ollama)
      â†“
Grounded Answer

ğŸ§© Modular Design (Single Responsibility)
Module	Responsibility
loader	Load & chunk raw text
vector_store	Vector DB creation & storage
retriever	Query orchestration
generator	Context-only answer generation
evaluator	Detect hallucinations
main	CLI application loop

Each module is replaceable without breaking the system.

âš™ï¸ Tech Stack (Actual)
Component	Technology
Embeddings	BAAI/bge-small-en-v1.5
Vector DB	ChromaDB
LLM	TinyLlama (via Ollama)
Language	Python
Hardware	CPU-only

No cloud. No paid APIs. Fully local.

ğŸ§ª Limitations (Honest)

Designed for smallâ€“medium corpora

TinyLlama has limited reasoning depth

Single-turn Q&A (no memory)

No reranking or hybrid retrieval

Basic similarity filtering

These are engineering tradeoffs, not bugs.

â–¶ï¸ How to Run
Prerequisites

Python 3.9+

Ollama installed

TinyLlama model pulled

ollama pull tinyllama

Setup
git clone https://github.com/<your-username>/mini_rag.git
cd mini_rag
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

Add Knowledge

Put your data in:

data/knowledge.txt

Ingest Data (One-Time)
python src/ingest.py

Start Q&A
python src/main.py


Ask questions related to knowledge.txt.
If the answer is not present, the system responds:

I don't know

Update Knowledge

If knowledge.txt changes:

rm -rf chroma_db
python src/ingest.py